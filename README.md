### avito-ranking-task
Ключевой фичей моей стратегии было использование большого объёма данных для расчёта семантического сходства с помощью SBERT, с применением модели paraphrase-multilingual-MiniLM-L12-v2, оптимизированной для русского языка.
К сожалению, из-за неверного планирования времени, пришлось представить показатель в 0,49 получившийся без учёта этой ключевой метрики в обучении. Крайняя попытка, которую я планировал представить, столкнулась с перегрузкой GPU, на финальном этапе, поэтому в работу были включены доступные на тот момент данные.
